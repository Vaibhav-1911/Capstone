{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1dQ64605aOR0tE_gcwUH6t0r2KWD8ILpO","timestamp":1684409446665}],"authorship_tag":"ABX9TyM6cHvE0999e2iGEyniTm80"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YPchpIwmglW4"},"outputs":[],"source":["import math, random\n","import torch\n","import torchaudio\n","from torchaudio import transforms\n","from IPython.display import Audio\n","\n","class AudioUtil():\n","  # ----------------------------\n","  # Load an audio file. Return the signal as a tensor and the sample rate\n","  # ----------------------------\n","  @staticmethod\n","  def open(audio_file):\n","    sig, sr = torchaudio.load(audio_file)\n","    return (sig, sr)\n","\n"," # ----------------------------\n","  # Convert the given audio to the desired number of channels\n","  # ----------------------------\n","  @staticmethod\n","  def rechannel(aud, new_channel):\n","      sig, sr = aud\n","\n","      if (sig.shape[0] == new_channel):\n","        ##############\n","          return aud\n","\n","      if (new_channel == 1):\n","        # Convert from stereo to mono by selecting only the first channel\n","          resig = sig[:1, :]\n","      else:\n","        # Convert from mono to stereo by duplicating the first channel\n","          resig = torch.cat([sig, sig])\n","\n","      return ((resig, sr))\n","\n","  # ----------------------------\n","    # Since Resample applies to a single channel, we resample one channel at a time\n","    # ----------------------------\n","  @staticmethod\n","  def resample(aud, newsr):\n","      sig, sr = aud\n","\n","      if (sr == newsr):\n","        #################\n","          return aud\n","\n","      num_channels = sig.shape[0]\n","      # Resample first channel\n","      resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n","      if (num_channels > 1):\n","        # Resample the second channel and merge both channels\n","          retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n","          resig = torch.cat([resig, retwo])\n","\n","      return ((resig, newsr))\n","\n","  # ----------------------------\n","    # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n","    # ----------------------------\n","  @staticmethod\n","  def pad_trunc(aud, max_ms):\n","      sig, sr = aud\n","      num_rows, sig_len = sig.shape\n","      max_len = sr//1000 * max_ms\n","\n","      if (sig_len > max_len):\n","        # Truncate the signal to the given length\n","          sig = sig[:,:max_len]\n","\n","      elif (sig_len < max_len):\n","        # Length of padding to add at the beginning and end of the signal\n","          pad_begin_len = random.randint(0, max_len - sig_len)\n","          pad_end_len = max_len - sig_len - pad_begin_len\n","\n","        # Pad with 0s\n","          pad_begin = torch.zeros((num_rows, pad_begin_len))\n","          pad_end = torch.zeros((num_rows, pad_end_len))\n","\n","          sig = torch.cat((pad_begin, sig, pad_end), 1)\n","        \n","      return (sig, sr)\n","\n","  # ----------------------------\n","    # Shifts the signal to the left or right by some percent. Values at the end\n","    # are 'wrapped around' to the start of the transformed signal.\n","    # ----------------------------\n","  @staticmethod\n","  def time_shift(aud, shift_limit):\n","      sig,sr = aud\n","      _, sig_len = sig.shape\n","      shift_amt = int(random.random() * shift_limit * sig_len)\n","      return (sig.roll(shift_amt), sr)\n","\n","  # ----------------------------\n","    # Generate a Spectrogram\n","    # ----------------------------\n","  @staticmethod\n","  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n","      sig,sr = aud\n","      top_db = 80\n","\n","      # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n","      spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n","\n","      # Convert to decibels\n","      spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n","      return (spec)\n","\n","  # ----------------------------\n","    # Augment the Spectrogram by masking out some sections of it in both the frequency\n","    # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n","    # overfitting and to help the model generalise better. The masked sections are\n","    # replaced with the mean value.\n","    # ----------------------------\n","  @staticmethod\n","  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n","      _, n_mels, n_steps = spec.shape\n","      mask_value = spec.mean()\n","      aug_spec = spec\n","\n","      freq_mask_param = max_mask_pct * n_mels\n","      for _ in range(n_freq_masks):\n","          aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n","\n","      time_mask_param = max_mask_pct * n_steps\n","      for _ in range(n_time_masks):\n","          aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n","\n","      return aug_spec"]},{"cell_type":"code","source":[],"metadata":{"id":"b8QizTk3hFJs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0PLNr25Nhevf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8-fIKfmZhxDk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k3JoQeHgin9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6DUcU_clivWh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GnI4ooebi-EK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, Dataset, random_split\n","import torchaudio\n","\n","# ----------------------------\n","# Sound Dataset\n","# ----------------------------\n","class SoundDS(Dataset):\n","    def __init__(self, df, data_path):\n","        self.df = df\n","        self.data_path = str(data_path)\n","        self.duration = 4000\n","        self.sr = 44100\n","        self.channel = 2\n","        self.shift_pct = 0.4\n","            \n","  # ----------------------------\n","  # Number of items in dataset\n","  # ----------------------------\n","    def __len__(self):\n","        return len(self.df)    \n","    \n","  # ----------------------------\n","  # Get i'th item in dataset\n","  # ----------------------------\n","    def __getitem__(self, idx):\n","        # Absolute file path of the audio file - concatenate the audio directory with\n","        # the relative path\n","        audio_file = self.data_path + self.df.loc[idx, 'full_file_path']\n","        # Get the Class ID\n","        class_id = self.df.loc[idx, 'class']\n","\n","        aud = AudioUtil.open(audio_file)\n","        # Some sounds have a higher sample rate, or fewer channels compared to the\n","        # majority. So we make all sounds have the same number of channels and same \n","        # sample rate. Unless the sample rate is the same, the pad_trunc will still\n","        # result in arrays of different lengths, even though the sound duration is\n","        # the same.\n","        reaud = AudioUtil.resample(aud, self.sr)\n","        rechan = AudioUtil.rechannel(reaud, self.channel)\n","\n","        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n","        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n","        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n","        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n","\n","        return aug_sgram, class_id\n"],"metadata":{"id":"VP1XOG7BjWhR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import librosa\n","import os\n","\n","# Mount Google Drive to access files\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"0CqUOBMauzH1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683357059716,"user_tz":240,"elapsed":4051,"user":{"displayName":"Vaibhav Gupta","userId":"16687362600075997253"}},"outputId":"ed947495-addf-465f-b988-5bcec36b3dd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Define the directories containing audio files\n","dir1 = '/content/drive/MyDrive/Capstone/data/emergency_sounds'\n","dir2 = '/content/drive/MyDrive/Capstone/data/general_sounds'\n","\n","# Create an empty list to store the file information\n","file_info = []\n","\n","# Loop through the files in each directory\n","for dir in [dir1, dir2]:\n","    for file in os.listdir(dir):\n","        if file.endswith('.wav'):  # Only consider .wav files\n","            # Load the audio file and compute its duration\n","            audio_path = os.path.join(dir, file)\n","            y, sr = librosa.load(audio_path, sr=None)\n","            duration = librosa.get_duration(y=y, sr=sr)\n","            \n","            # Store the file information in a dictionary\n","            file_dict = {\n","                'filename': file,\n","                'duration': duration,\n","                'class': os.path.basename(dir),\n","                'folder_path': dir,\n","                'full_file_path': audio_path\n","            }\n","            \n","            # Add the file information to the list\n","            file_info.append(file_dict)\n","\n","# Create a DataFrame from the file information list\n","df = pd.DataFrame(file_info)\n","\n","# Print the DataFrame\n","print(df)"],"metadata":{"id":"DNH82rogutEe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683357089003,"user_tz":240,"elapsed":29289,"user":{"displayName":"Vaibhav Gupta","userId":"16687362600075997253"}},"outputId":"c53e4c71-4d7a-4664-b58d-cbeaf3567b6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                filename   duration             class  \\\n","0                    siren-firetruck.wav  23.328542  emergency_sounds   \n","1            distant-ambulance-siren.wav  24.000000  emergency_sounds   \n","2                   german-ambulance.wav  16.467528  emergency_sounds   \n","3                ambulance-in-oradea.wav  25.000000  emergency_sounds   \n","4    one-police-siren-with-long-tail.wav  53.050023  emergency_sounds   \n","..                                   ...        ...               ...   \n","215              xmas-bellis2-120bpm.wav  34.063688    general_sounds   \n","216       Woodpecker Pecking on Tree.wav   5.564083    general_sounds   \n","217             water-at-the-beach-2.wav  29.561021    general_sounds   \n","218                       Woodpecker.wav   1.724083    general_sounds   \n","219          Woodpecker Pecking Fast.wav   1.985313    general_sounds   \n","\n","                                           folder_path  \\\n","0    /content/drive/MyDrive/Capstone/data/emergency...   \n","1    /content/drive/MyDrive/Capstone/data/emergency...   \n","2    /content/drive/MyDrive/Capstone/data/emergency...   \n","3    /content/drive/MyDrive/Capstone/data/emergency...   \n","4    /content/drive/MyDrive/Capstone/data/emergency...   \n","..                                                 ...   \n","215  /content/drive/MyDrive/Capstone/data/general_s...   \n","216  /content/drive/MyDrive/Capstone/data/general_s...   \n","217  /content/drive/MyDrive/Capstone/data/general_s...   \n","218  /content/drive/MyDrive/Capstone/data/general_s...   \n","219  /content/drive/MyDrive/Capstone/data/general_s...   \n","\n","                                        full_file_path  \n","0    /content/drive/MyDrive/Capstone/data/emergency...  \n","1    /content/drive/MyDrive/Capstone/data/emergency...  \n","2    /content/drive/MyDrive/Capstone/data/emergency...  \n","3    /content/drive/MyDrive/Capstone/data/emergency...  \n","4    /content/drive/MyDrive/Capstone/data/emergency...  \n","..                                                 ...  \n","215  /content/drive/MyDrive/Capstone/data/general_s...  \n","216  /content/drive/MyDrive/Capstone/data/general_s...  \n","217  /content/drive/MyDrive/Capstone/data/general_s...  \n","218  /content/drive/MyDrive/Capstone/data/general_s...  \n","219  /content/drive/MyDrive/Capstone/data/general_s...  \n","\n","[220 rows x 5 columns]\n"]}]},{"cell_type":"code","source":["df = df[[\"full_file_path\", \"class\"]]"],"metadata":{"id":"YC9sK5QLwFtx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['class'] = df['class'].map({'emergency_sounds': 1, 'general_sounds': 0})"],"metadata":{"id":"vTe5smjtxE5B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"S_9ogW7_xn8T","colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"status":"ok","timestamp":1683357089004,"user_tz":240,"elapsed":22,"user":{"displayName":"Vaibhav Gupta","userId":"16687362600075997253"}},"outputId":"48ab5e12-6a87-4cf2-fdcf-b01297116b9d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                        full_file_path  class\n","0    /content/drive/MyDrive/Capstone/data/emergency...      1\n","1    /content/drive/MyDrive/Capstone/data/emergency...      1\n","2    /content/drive/MyDrive/Capstone/data/emergency...      1\n","3    /content/drive/MyDrive/Capstone/data/emergency...      1\n","4    /content/drive/MyDrive/Capstone/data/emergency...      1\n","..                                                 ...    ...\n","215  /content/drive/MyDrive/Capstone/data/general_s...      0\n","216  /content/drive/MyDrive/Capstone/data/general_s...      0\n","217  /content/drive/MyDrive/Capstone/data/general_s...      0\n","218  /content/drive/MyDrive/Capstone/data/general_s...      0\n","219  /content/drive/MyDrive/Capstone/data/general_s...      0\n","\n","[220 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-6a17659e-a03e-4f16-a3da-8551eb1d7ac4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>full_file_path</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/MyDrive/Capstone/data/emergency...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/MyDrive/Capstone/data/emergency...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/MyDrive/Capstone/data/emergency...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/MyDrive/Capstone/data/emergency...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/MyDrive/Capstone/data/emergency...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>215</th>\n","      <td>/content/drive/MyDrive/Capstone/data/general_s...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>216</th>\n","      <td>/content/drive/MyDrive/Capstone/data/general_s...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>217</th>\n","      <td>/content/drive/MyDrive/Capstone/data/general_s...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>218</th>\n","      <td>/content/drive/MyDrive/Capstone/data/general_s...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>219</th>\n","      <td>/content/drive/MyDrive/Capstone/data/general_s...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>220 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a17659e-a03e-4f16-a3da-8551eb1d7ac4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6a17659e-a03e-4f16-a3da-8551eb1d7ac4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6a17659e-a03e-4f16-a3da-8551eb1d7ac4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["from torch.utils.data import random_split\n","data_path = \"\"\n","myds = SoundDS(df, data_path)\n","\n","# Random split of 80:20 between training and validation\n","num_items = len(myds)\n","num_train = round(num_items * 0.8)\n","num_val = num_items - num_train\n","train_ds, val_ds = random_split(myds, [num_train, num_val])\n","\n","# Create training and validation data loaders\n","train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n","val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"],"metadata":{"id":"cERhzhGojWe9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_ds"],"metadata":{"id":"7VRaNoOKRWDZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683357089005,"user_tz":240,"elapsed":10,"user":{"displayName":"Vaibhav Gupta","userId":"16687362600075997253"}},"outputId":"08e1e60d-41b0-4473-9493-3a5f155cbfde"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataset.Subset at 0x7ff0bf6a4580>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import torch.nn.functional as F\n","from torch.nn import init\n","import torch.nn as nn\n","\n","# ----------------------------\n","# Audio Classification Model\n","# ----------------------------\n","class AudioClassifier (nn.Module):\n","    # ----------------------------\n","    # Build the model architecture\n","    # ----------------------------\n","    def __init__(self):\n","        super().__init__()\n","        conv_layers = []\n","\n","        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n","        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n","        self.relu1 = nn.ReLU()\n","        self.bn1 = nn.BatchNorm2d(8)\n","        init.kaiming_normal_(self.conv1.weight, a=0.1)\n","        self.conv1.bias.data.zero_()\n","        conv_layers += [self.conv1, self.relu1, self.bn1]\n","\n","        # Second Convolution Block\n","        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu2 = nn.ReLU()\n","        self.bn2 = nn.BatchNorm2d(16)\n","        init.kaiming_normal_(self.conv2.weight, a=0.1)\n","        self.conv2.bias.data.zero_()\n","        conv_layers += [self.conv2, self.relu2, self.bn2]\n","\n","        # Third Convolution Block\n","        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu3 = nn.ReLU()\n","        self.bn3 = nn.BatchNorm2d(32)\n","        init.kaiming_normal_(self.conv3.weight, a=0.1)\n","        self.conv3.bias.data.zero_()\n","        conv_layers += [self.conv3, self.relu3, self.bn3]\n","\n","        # Fourth Convolution Block\n","        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu4 = nn.ReLU()\n","        self.bn4 = nn.BatchNorm2d(64)\n","        init.kaiming_normal_(self.conv4.weight, a=0.1)\n","        self.conv4.bias.data.zero_()\n","        conv_layers += [self.conv4, self.relu4, self.bn4]\n","\n","        # Linear Classifier\n","        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n","        self.lin = nn.Linear(in_features=64, out_features=10)\n","\n","        # Wrap the Convolutional Blocks\n","        self.conv = nn.Sequential(*conv_layers)\n"," \n","    # ----------------------------\n","    # Forward pass computations\n","    # ----------------------------\n","    def forward(self, x):\n","        # Run the convolutional blocks\n","        x = self.conv(x)\n","\n","        # Adaptive pool and flatten for input to linear layer\n","        x = self.ap(x)\n","        x = x.view(x.shape[0], -1)\n","\n","        # Linear layer\n","        x = self.lin(x)\n","\n","        # Final output\n","        return x\n","\n","# Create the model and put it on the GPU if available\n","model = AudioClassifier()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","# Check that it is on Cuda\n","next(model.parameters()).device"],"metadata":{"id":"I64XevIHjWbz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683357089005,"user_tz":240,"elapsed":8,"user":{"displayName":"Vaibhav Gupta","userId":"16687362600075997253"}},"outputId":"5f0c8758-da3e-4ec9-dfd3-0b64920d227c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["import pickle\n","\n","# ----------------------------\n","# evaluation loop\n","# ----------------------------\n","def evaluate(model, val_dl, criterion):\n","    # Set model to evaluation mode\n","    model.eval()\n","\n","    # Keep track of total loss\n","    total_loss = 0.0\n","    correct_prediction = 0\n","    total_prediction = 0\n","\n","    # Disable gradient computation to save memory and computation time\n","    with torch.no_grad():\n","        # Iterate over validation set batches\n","        for data in val_dl:\n","            # Get the input features and target labels, and put them on the GPU\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            # Normalize the inputs\n","            inputs_m, inputs_s = inputs.mean(), inputs.std()\n","            inputs = (inputs - inputs_m) / inputs_s\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            # Add batch loss to total loss\n","            total_loss += loss.item()\n","\n","            # Get the predicted class with the highest score\n","            _, prediction = torch.max(outputs,1)\n","            # Count of predictions that matched the target label\n","            correct_prediction += (prediction == labels).sum().item()\n","            total_prediction += prediction.shape[0]\n","\n","    # Calculate average validation loss and accuracy\n","    avg_loss = total_loss / len(val_dl)\n","    acc = correct_prediction/total_prediction\n","    # Set model back to training mode\n","    model.train()\n","\n","    return avg_loss, acc\n","\n","\n","\n","# ----------------------------\n","# Training Loop\n","# ----------------------------\n","def training(model, train_dl, num_epochs):\n","  # Loss Function, Optimizer and Scheduler\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n","  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n","                                                steps_per_epoch=int(len(train_dl)),\n","                                                epochs=num_epochs,\n","                                                anneal_strategy='linear')\n","  \n","  # Variables to keep track of best validation loss, best validation accuracy and corresponding model\n","  best_val_acc = -float('inf')\n","  best_val_loss = float('inf')\n","  best_model_state = None\n","\n","  # Repeat for each epoch\n","  for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    correct_prediction = 0\n","    total_prediction = 0\n","\n","    # Repeat for each batch in the training set\n","    for i, data in enumerate(train_dl):\n","        # Get the input features and target labels, and put them on the GPU\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        # Normalize the inputs\n","        inputs_m, inputs_s = inputs.mean(), inputs.std()\n","        inputs = (inputs - inputs_m) / inputs_s\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        # Keep stats for Loss and Accuracy\n","        running_loss += loss.item()\n","\n","        # Get the predicted class with the highest score\n","        _, prediction = torch.max(outputs,1)\n","        # Count of predictions that matched the target label\n","        correct_prediction += (prediction == labels).sum().item()\n","        total_prediction += prediction.shape[0]\n","\n","        #if i % 10 == 0:    # print every 10 mini-batches\n","        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n","    \n","    # Calculate validation loss and save model if it's the best one so far\n","    val_loss = evaluate(model, val_dl, criterion)[0]\n","    val_acc = evaluate(model, val_dl, criterion)[1]\n","    if (val_acc >= best_val_acc and val_loss <= best_val_loss):\n","        best_val_acc = val_acc\n","        best_val_loss = val_loss\n","        best_model = model\n","    \n","    # Print stats at the end of the epoch\n","    num_batches = len(train_dl)\n","    avg_loss = running_loss / num_batches\n","    acc = correct_prediction/total_prediction\n","    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}, Val_Loss: {val_loss:.2f}, Val_Accuracy: {val_acc:.2f}')\n","\n","    # Save the best model to a pickle file\n","  with open('/content/drive/MyDrive/Capstone/best_model.pkl', 'wb') as f:\n","    pickle.dump(best_model, f)\n","  f.close()\n","num_epochs=20   # adjust as per requirement\n","training(model, train_dl, num_epochs)"],"metadata":{"id":"hO-5TWVGN-ND","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683357830152,"user_tz":240,"elapsed":741153,"user":{"displayName":"Vaibhav Gupta","userId":"16687362600075997253"}},"outputId":"40ab34ce-0b75-4045-b68b-6a82e95564a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Loss: 2.30, Accuracy: 0.11, Val_Loss: 2.26, Val_Accuracy: 0.16\n","Epoch: 1, Loss: 2.27, Accuracy: 0.16, Val_Loss: 2.21, Val_Accuracy: 0.23\n","Epoch: 2, Loss: 2.20, Accuracy: 0.38, Val_Loss: 2.09, Val_Accuracy: 0.57\n","Epoch: 3, Loss: 2.16, Accuracy: 0.53, Val_Loss: 2.06, Val_Accuracy: 0.59\n","Epoch: 4, Loss: 2.09, Accuracy: 0.58, Val_Loss: 2.03, Val_Accuracy: 0.59\n","Epoch: 5, Loss: 1.99, Accuracy: 0.60, Val_Loss: 1.95, Val_Accuracy: 0.61\n","Epoch: 6, Loss: 1.89, Accuracy: 0.62, Val_Loss: 1.88, Val_Accuracy: 0.61\n","Epoch: 7, Loss: 1.73, Accuracy: 0.65, Val_Loss: 1.82, Val_Accuracy: 0.70\n","Epoch: 8, Loss: 1.57, Accuracy: 0.69, Val_Loss: 1.58, Val_Accuracy: 0.64\n","Epoch: 9, Loss: 1.46, Accuracy: 0.70, Val_Loss: 1.51, Val_Accuracy: 0.64\n","Epoch: 10, Loss: 1.34, Accuracy: 0.69, Val_Loss: 1.39, Val_Accuracy: 0.66\n","Epoch: 11, Loss: 1.29, Accuracy: 0.71, Val_Loss: 1.38, Val_Accuracy: 0.52\n","Epoch: 12, Loss: 1.19, Accuracy: 0.69, Val_Loss: 1.33, Val_Accuracy: 0.66\n","Epoch: 13, Loss: 1.15, Accuracy: 0.72, Val_Loss: 1.16, Val_Accuracy: 0.68\n","Epoch: 14, Loss: 0.96, Accuracy: 0.78, Val_Loss: 1.09, Val_Accuracy: 0.70\n","Epoch: 15, Loss: 0.96, Accuracy: 0.77, Val_Loss: 1.07, Val_Accuracy: 0.64\n","Epoch: 16, Loss: 0.94, Accuracy: 0.77, Val_Loss: 1.11, Val_Accuracy: 0.59\n","Epoch: 17, Loss: 0.88, Accuracy: 0.80, Val_Loss: 1.03, Val_Accuracy: 0.70\n","Epoch: 18, Loss: 0.88, Accuracy: 0.77, Val_Loss: 0.98, Val_Accuracy: 0.61\n","Epoch: 19, Loss: 0.90, Accuracy: 0.76, Val_Loss: 0.94, Val_Accuracy: 0.68\n"]}]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/Capstone/best_model.pkl', 'rb') as f:\n","    model = pickle.load(f)"],"metadata":{"id":"8Sqwc2Ed7cJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the directories containing unseen audio files\n","dir3 = '/content/drive/MyDrive/Capstone/data/unseen/unseen_emergency'\n","dir4 = '/content/drive/MyDrive/Capstone/data/unseen/unseen_general'\n","\n","# Create an empty list to store the file information\n","unseen_file_info = []\n","\n","# Loop through the files in each directory\n","for dir in [dir3, dir4]:\n","    for file in os.listdir(dir):\n","        if file.endswith('.wav'):  # Only consider .wav files\n","            # Load the audio file and compute its duration\n","            audio_path = os.path.join(dir, file)\n","            y, sr = librosa.load(audio_path, sr=None)\n","            duration = librosa.get_duration(y=y, sr=sr)\n","            \n","            # Store the file information in a dictionary\n","            unseen_file_dict = {\n","                'filename': file,\n","                'duration': duration,\n","                'class': os.path.basename(dir),\n","                'folder_path': dir,\n","                'full_file_path': audio_path\n","            }\n","            \n","            # Add the file information to the list\n","            unseen_file_info.append(unseen_file_dict)\n","\n","# Create a DataFrame from the file information list\n","unseen_df = pd.DataFrame(unseen_file_info)\n","\n","# Print the DataFrame\n","print(unseen_df)"],"metadata":{"id":"RxwN3jRbOh_Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683358029562,"user_tz":240,"elapsed":310,"user":{"displayName":"Vaibhav Gupta","userId":"16687362600075997253"}},"outputId":"259ca873-8961-46c4-bc80-bb12c760d282"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                  filename   duration             class  \\\n","0    emergency-alarm-with-reverb-29431.wav  16.056000  unseen_emergency   \n","1                female-scream-1-86768.wav   2.690612  unseen_emergency   \n","2                  angry-mob-loop-6847.wav  11.880000  unseen_emergency   \n","3                    dog-barking-70772.wav  16.392000  unseen_emergency   \n","4                    man-scream-121085.wav   3.657143  unseen_emergency   \n","5   9mm-pistol-shoot-short-reverb-7152.wav   1.488980  unseen_emergency   \n","6                 9mm-pistol-shot-6349.wav   1.697959  unseen_emergency   \n","7                   cafe-ambience-9263.wav  20.427755    unseen_general   \n","8                   crowd_talking-6762.wav  89.904000    unseen_general   \n","9                child-laughing-113112.wav  10.057143    unseen_general   \n","10              short-crowd-cheer-6713.wav   7.944000    unseen_general   \n","11                 laughing-man-117725.wav   4.414694    unseen_general   \n","12               possessed-laugh-94851.wav   4.872000    unseen_general   \n","13               crowd-cheering-143103.wav   6.347755    unseen_general   \n","\n","                                          folder_path  \\\n","0   /content/drive/MyDrive/Capstone/data/unseen/un...   \n","1   /content/drive/MyDrive/Capstone/data/unseen/un...   \n","2   /content/drive/MyDrive/Capstone/data/unseen/un...   \n","3   /content/drive/MyDrive/Capstone/data/unseen/un...   \n","4   /content/drive/MyDrive/Capstone/data/unseen/un...   \n","5   /content/drive/MyDrive/Capstone/data/unseen/un...   \n","6   /content/drive/MyDrive/Capstone/data/unseen/un...   \n","7   /content/drive/MyDrive/Capstone/data/unseen/un...   \n","8   /content/drive/MyDrive/Capstone/data/unseen/un...   \n","9   /content/drive/MyDrive/Capstone/data/unseen/un...   \n","10  /content/drive/MyDrive/Capstone/data/unseen/un...   \n","11  /content/drive/MyDrive/Capstone/data/unseen/un...   \n","12  /content/drive/MyDrive/Capstone/data/unseen/un...   \n","13  /content/drive/MyDrive/Capstone/data/unseen/un...   \n","\n","                                       full_file_path  \n","0   /content/drive/MyDrive/Capstone/data/unseen/un...  \n","1   /content/drive/MyDrive/Capstone/data/unseen/un...  \n","2   /content/drive/MyDrive/Capstone/data/unseen/un...  \n","3   /content/drive/MyDrive/Capstone/data/unseen/un...  \n","4   /content/drive/MyDrive/Capstone/data/unseen/un...  \n","5   /content/drive/MyDrive/Capstone/data/unseen/un...  \n","6   /content/drive/MyDrive/Capstone/data/unseen/un...  \n","7   /content/drive/MyDrive/Capstone/data/unseen/un...  \n","8   /content/drive/MyDrive/Capstone/data/unseen/un...  \n","9   /content/drive/MyDrive/Capstone/data/unseen/un...  \n","10  /content/drive/MyDrive/Capstone/data/unseen/un...  \n","11  /content/drive/MyDrive/Capstone/data/unseen/un...  \n","12  /content/drive/MyDrive/Capstone/data/unseen/un...  \n","13  /content/drive/MyDrive/Capstone/data/unseen/un...  \n"]}]},{"cell_type":"code","source":["unseen_df = unseen_df[[\"full_file_path\", \"class\"]]"],"metadata":{"id":"WXRLkzYtOvPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unseen_df['class'] = unseen_df['class'].map({'unseen_emergency': 1, 'unseen_general': 0})"],"metadata":{"id":"7etdMdmGO4H5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df = SoundDS(unseen_df, data_path)"],"metadata":{"id":"8_SXii4uPzYz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dl = torch.utils.data.DataLoader(test_df, batch_size=16, shuffle=False)"],"metadata":{"id":"YhUpE_Y-QUU4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ----------------------------\n","# Inference\n","# ----------------------------\n","def inference (model, test_dl):\n","  correct_prediction = 0\n","  total_prediction = 0\n","\n","  # Disable gradient updates\n","  with torch.no_grad():\n","    for data in test_dl:\n","      # Get the input features and target labels, and put them on the GPU\n","      inputs, labels = data[0].to(device), data[1].to(device)\n","\n","      # Normalize the inputs\n","      inputs_m, inputs_s = inputs.mean(), inputs.std()\n","      inputs = (inputs - inputs_m) / inputs_s\n","\n","      # Get predictions\n","      outputs = model(inputs)\n","\n","      # Get the predicted class with the highest score\n","      _, prediction = torch.max(outputs,1)\n","      # Count of predictions that matched the target label\n","      correct_prediction += (prediction == labels).sum().item()\n","      total_prediction += prediction.shape[0]\n","    \n","  acc = correct_prediction/total_prediction\n","  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n","\n","# Run inference on trained model with the validation set\n","inference(model, test_dl)"],"metadata":{"id":"LF6i08B0OB31","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683358127281,"user_tz":240,"elapsed":630,"user":{"displayName":"Vaibhav Gupta","userId":"16687362600075997253"}},"outputId":"55d1ebc1-a0f6-405b-cd75-014b9ced175a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.43, Total items: 14\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","y_true = []\n","y_pred = []\n","with torch.no_grad():\n","    for inputs, labels in test_dl:\n","        # Forward pass\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        # Save true and predicted labels\n","        y_true.extend(labels.tolist())\n","        y_pred.extend(predicted.tolist())\n","\n","report = classification_report(y_true, y_pred)\n","print(report)"],"metadata":{"id":"sCMIsAoFQvEW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683358130650,"user_tz":240,"elapsed":597,"user":{"displayName":"Vaibhav Gupta","userId":"16687362600075997253"}},"outputId":"4aae1da7-7d66-4f88-cb1a-bbe7f0dd6ebf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.71      0.71      0.71         7\n","           1       0.71      0.71      0.71         7\n","\n","    accuracy                           0.71        14\n","   macro avg       0.71      0.71      0.71        14\n","weighted avg       0.71      0.71      0.71        14\n","\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","cm = confusion_matrix(y_true, y_pred)\n","\n","sns.heatmap(cm, annot=True, cmap=\"Blues\")\n","plt.xlabel(\"Predicted labels\")\n","plt.ylabel(\"True labels\")\n","plt.show()\n"],"metadata":{"id":"V6zfFHf1TLM0","colab":{"base_uri":"https://localhost:8080/","height":455},"executionInfo":{"status":"ok","timestamp":1683358146660,"user_tz":240,"elapsed":443,"user":{"displayName":"Vaibhav Gupta","userId":"16687362600075997253"}},"outputId":"986f50e4-ba85-42a2-93e0-484aa86d1390"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]}]}